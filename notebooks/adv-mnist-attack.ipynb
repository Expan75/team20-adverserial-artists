{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1187,
   "id": "a1c338fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import random\n",
    "import hashlib\n",
    "import dataclasses\n",
    "from abc import ABC\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "from typing import Optional, Any, Dict, List, Tuple, Callable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 952,
   "id": "96ae2401-1271-45b9-bd29-936a637abaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent = os.path.dirname(os.getcwd())\n",
    "if \"notebooks\" in parent:\n",
    "    parent = os.path.dirname(parent)\n",
    "sys.path.append(parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 953,
   "id": "418a066c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75025"
      ]
     },
     "execution_count": 953,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from common import example\n",
    "\n",
    "# test run an import to ensure we can import\n",
    "example.memoed_fib(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4466b73",
   "metadata": {},
   "source": [
    "# Baseline MNIST classifier (to be attacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 958,
   "id": "66f8ef58",
   "metadata": {},
   "outputs": [],
   "source": [
    "KERAS_DIRECTORY = os.path.expanduser(\"~/.keras/datasets\")\n",
    "MNIST_FILEPATH = os.path.join(KERAS_DIRECTORY, \"mnist.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1378,
   "id": "dcc072b0-d5cc-4e24-92bd-d0f02950f579",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTSubset(Dataset):\n",
    "    \"\"\"Original data is already split so that's the view of dataset we're represent\"\"\"\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.x[idx, :, :], self.y[idx]\n",
    "\n",
    "    def loader(self, batch_size: int = 32, shuffle: bool = True) -> DataLoader:\n",
    "        return DataLoader(self, shuffle=shuffle, batch_size=batch_size)\n",
    "\n",
    "    # if time polish this\n",
    "    def display(self, images=2): \n",
    "        labels = len(sorted(np.unique(self.y)))\n",
    "        fig, axs = plt.subplots(labels, images, tight_layout=True)\n",
    "        \n",
    "        for label_i in range(labels):\n",
    "            for image_j in range(images):\n",
    "                # not pretty\n",
    "                image, label = self[random.randint(0, len(self.x))]\n",
    "                if label != label_i:\n",
    "                    image, label = self[random.randint(0, len(self.x))]\n",
    "                axs[label, image_j].imshow(image.reshape(28, 28))\n",
    "                axs[label, image_j].set_title(int(label))\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "class InMemoryMNIST:\n",
    "\n",
    "    data = {}\n",
    "    test: MNISTSubset\n",
    "    train: MNISTSubset\n",
    "\n",
    "    def __init__(self, filepath: str):\n",
    "        for key, data in dict(np.load(filepath)).items():\n",
    "            # labels need to be encoded as longs, rest floats\n",
    "            data = torch.as_tensor(data, dtype=torch.float32 if \"x\" in key else torch.long)\n",
    "            if \"x\" in key:\n",
    "                # torch conv layers require images of shape (idx, channels, width, height);\n",
    "                data = data / np.linalg.norm(data)\n",
    "                data = data.reshape(data.shape[0], 1, 28, 28)\n",
    "            \n",
    "            self.data[key] = data\n",
    "        \n",
    "        self.train = MNISTSubset(self.data[\"x_train\"], self.data[\"y_train\"])\n",
    "        self.test = MNISTSubset(self.data[\"x_train\"], self.data[\"y_test\"])\n",
    "\n",
    "    \n",
    "    def loaders(self) -> (DataLoader, DataLoader):\n",
    "        return self.train.loader(), self.test.loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1379,
   "id": "8a3e0799-4b0a-4d5b-8521-3f51f2a25fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = InMemoryMNIST(MNIST_FILEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1380,
   "id": "ceb54f9e-86dc-406d-b534-32c70afd118c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(ground_truth: DataLoader, model: nn.Module, *args, **kwargs) -> float:\n",
    "\n",
    "    examples = 0\n",
    "    correctly_labeled = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        correctly_labeled = 0\n",
    "        for batch in ground_truth:\n",
    "            images, true_labels = batch\n",
    "            pred_labels = model(images).argmax(dim=1)\n",
    "\n",
    "            examples += len(pred_labels)\n",
    "            correctly_labeled += torch.sum(true_labels == pred_labels)\n",
    "    \n",
    "    return int(correctly_labeled) / examples\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Experiment:\n",
    "    description: str = \"\"\n",
    "    epochs: int = 100\n",
    "    loss: Callable = torch.nn.CrossEntropyLoss\n",
    "    optimizer: Callable = partial(torch.optim.Adam, lr=0.01, weight_decay=5e-4)\n",
    "    \n",
    "    metrics: Dict[str, Callable] = dataclasses.field(default_factory=dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1381,
   "id": "ea6d7d2c-6f75-44e0-b5dd-15600da69502",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseCNNClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=(2, 2), stride=2)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2), stride=1)\n",
    "        self.conv2 = nn.Conv2d(10, 1, kernel_size=(2, 2), stride=2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(25, 10)\n",
    "        self.fc2 = nn.Linear(10, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool1(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.softmax(self.fc2(x), 1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1386,
   "id": "be12005c-5211-46ed-a3ad-da927edf7b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: nn.Module,\n",
    "    test_set: DataLoader,\n",
    "    training_set: DataLoader, \n",
    "    params: Experiment\n",
    "):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    with tqdm(range(1, params.epochs + 1), desc=\"epoch training loss\", leave=True) as epochs:\n",
    "        for epoch in epochs:\n",
    "            model.train()\n",
    "            for batch in training_set:\n",
    "                optimizer.zero_grad()\n",
    "                images, true_labels = batch\n",
    "                prob = model(images)\n",
    "                loss = loss_func(prob, true_labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # evaluate on test set\n",
    "            model.eval()\n",
    "            metrics = { metric: f(test_set, model) for metric, f in params.metrics.items() }\n",
    "            epochs.set_description(f\"{epoch=}, {loss=:.4f}, {metrics=}\")\n",
    "    \n",
    "    return model, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d8da15-7c78-4681-900d-3a45aea38759",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch=14, loss=2.2857, metrics={'accuracy': 0.1135}:   7%| | 14/200 [02:02<25:21"
     ]
    }
   ],
   "source": [
    "mnist_train, mnist_test = mnist.loaders()\n",
    "\n",
    "model = BaseCNNClassifier()\n",
    "train(model, mnist_test, mnist_train, Experiment(epochs=200, metrics={\"accuracy\": accuracy}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38caa5fa-cab4-4829-a9cc-9e4a5a7358c0",
   "metadata": {},
   "source": [
    "## TODO: repurpose classifier and attack adverially by noise injection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
